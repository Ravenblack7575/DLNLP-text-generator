### DLNLP-text-generator
# SDGAI DLNLP Assignment - Making a Text Generator using Recurrent Neural Networks


Project summary generated by NotebookLM:

### Introduction

The primary aim of this project was **to build and train an RNN-based model capable of generating semi-coherent English sentences from scratch**. The model was specifically trained on the text content from a book, with the expectation that the generated output would reproduce text resembling the style and content (including names and fictional objects) of the source material. The core approach involved processing the source text, building and training the RNN model for character-by-character generation, and evaluating the final models using both intrinsic metrics and human assessment.

### Methods

The methodology focused on using Long Short Term Memory (LSTM) layers, a design suitable for retaining sequence information needed to predict the next character. The source text, initially possessing a corpus length of 676,978 characters, underwent extensive cleanup, including removing line breaks, special characters, and repetitive footer text (such as the author's name and book title, which were removed after initial models generated them frequently). The cleaned data had a final corpus size of 607,752 characters. The data was one-hot encoded and segmented into sequences of 100 characters, later increased to 160 characters (maxlen), resulting in 202,531 training sequences. During training, various model versions were optimized by adjusting LSTM layer counts, unit sizes, dropout rates, and hyperparameters, with the **Adam optimizer** ultimately replacing RMSprop due to improved performance.

### Results

The project concluded with the evaluation of two "best" models: **Model 6-4** (two stacked LSTM layers) and **Model 6-6** (three stacked LSTM layers). Text generation quality was evaluated based on metrics such as unique word ratio, repetition rate, and the **accuracy ratio** (the number of actual English words out of all generated words). Model 6-6 generally achieved slightly higher accuracy ratios and was assessed by human judgment as generating text that was more readable than Model 6-4. Despite successful training and optimization, the results showed that **neither model was able to generate full sentences that maintained logical or linguistic sense**. It was observed that increasing the temperature setting during generation resulted in greater word diversity but decreased the accuracy ratio (producing more nonsense words), while very low temperatures often led to repetitive word sequences. A demonstration of the two final models, converted into a Gradio app using Claude AI, is hosted on **HuggingFace Spaces**.


====================================================================

Demo available at HuggingFace Spaces: https://huggingface.co/spaces/Ravenblack7575/harrypottertextgen
